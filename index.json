[{"categories":null,"content":"1 深度神经网络训练过程 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:1:0","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"1.1 深度神经网络 深度神经网络由若干层堆叠而成，每层由如下式子计算： $$\\boldsymbol{z = W \\cdot x + b}$$ $$\\boldsymbol{a} = f(\\boldsymbol{z})$$ 其中，$\\boldsymbol{x}$ 是输入，$\\boldsymbol{W}$ 和 $\\boldsymbol{b}$ 是权重，$f$ 是激活函数（Activation Function）。 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:1:1","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"1.2 前向传播 由输入经过多层神经网络到输出的计算过程。 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:1:2","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"1.3 反向传播 从后向前更新各层的权重的过程。 损失函数（Loss Function）$\\boldsymbol{L}$ 计算前向传播的输出值与真实值之间的差距。利用链式法则从后往前计算损失函数对每层权重的偏导数（梯度），并按照如下公式更新权重： $$\\boldsymbol{W = W - \\alpha \\dfrac{\\partial{L}}{\\partial{W}}}$$ $$\\boldsymbol{b = b - \\alpha \\dfrac{\\partial{L}}{\\partial{b}}}$$ 其中，$\\alpha$ 是学习率，表示权重更新的速度。 权重沿着梯度方向下降，因此又称为梯度下降法（Gradient decent）。 注意，在应用链式法则计算权重梯度时，对当前层权重梯度的计算可转化为当前层输出的函数，即： $$\\boldsymbol{\\dfrac{\\partial{L}}{\\partial{W_l}} = \\delta a_l}$$ 所以，反向传播计算依赖于前向传播过程中计算并保存的中间层输出。 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:1:3","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"1.X 参考 2.3. 深度学习——Python数据科学加速 一文弄懂神经网络中的反向传播法——BackPropagation ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:1:4","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"2 分布式训练 分布式训练（Distributed Training）是指将训练任务分解成多个子任务，并在多个计算设备上并行地进行训练。 分布式训练系统的目标就是将单节点模型训练转换成等价的分布式并行模型训练。对于大语言模型来说，训练过程就是根据数据和损失函数，利用优化算法对神经网络模型参数进行更新的过程。 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:2:0","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"2.1 3D并行 2.1.1 数据并行（DP） 将同样的模型复制到多个计算设备上，每个计算设备都拥有一个模型副本（Model replica）。计算时，每个计算设备处理输入batch的一个子集，相当于沿batch维度对训练过程进行并行化。假设输入batch size 为N，计算设备数量为M，则每个计算设备会分配到N/M个样本。 每个模型副本使用分配到的batch进行前向计算，前向计算完成后，每个计算设备会根据本地的样本计算损失，得到梯度 $\\boldsymbol{G_i}$ （ $i$ 为设备编号），并将本地梯度进行广播。所有计算设备需要聚合其他设备的梯度值，计算出平均梯度 $(\\sum_{i = 1}^{N}\\boldsymbol{G_i}) / N$ 来更新参数，完成这个batch的训练。 与单设备训练相比，数据并行训练最主要的区别在于反向计算中的梯度需要在所有计算设备之间同步，以保证每个计算设备上最终得到的是所有模型副本上梯度的平均值。由于基于Transformer架构的大语言模型中每个算子都是依赖单个数据而非整个批次数据，因此，数据并行并不会影响其计算逻辑。一般情况下各计算设备中前向计算是相互独立的，不涉及同步问题。 数据并行的优缺点 数据并行可以通过增加计算设备数量，提升整体训练吞吐量，提升gbsps（Global Batch Size per Second），具有很高的加速比，但需要在每个计算设备上加载一份模型副本，显存占用较高。 2.1.2 张量并行（TP） 张量并行通过将模型的张量操作分割成多个子操作，并将这些子操作分配到不同的计算设备上，从而实现并行计算。 张量并行优缺点 与传统的模型并行相比，张量并行的粒度更细，可以在更高效利用计算资源的同时减少通信开销。 2.1.3 流水线并行（PP） 将模型的层拆分到多个计算设备上进行计算，每个设备上分得的层称为一个stage。流水线并行需要在计算节点之间通信，将前一个stage的输出作为后一个stage的输入，使得前后stage能够流水式地、分批地进行工作。流水线并行将模型分成多个stages，每个stage分配到不同的计算设备上，然后将数据以流水线的方式在各个设备之间传递，从而实现并行计算。 每个stage包含模型的一部分层或子模块，输入数据的mini-batches被细分成多个micro-batches，每个micro batch在流水线的一个stage上进行计算，然后依次传递到下一个stage。 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:2:1","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"2.X 参考 理论+实践，带你了解分布式训练 ","date":"2025-01-13","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/:2:2","tags":["distruibuted training"],"title":"分布式训练","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"categories":null,"content":"本文简要记录使用Github Pages与Hugo构建个人博客的流程。 ","date":"2025-01-12","objectID":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:0:0","tags":["tools"],"title":"使用Hugo构建个人博客","uri":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"1 创建Github Pages Repo 这个步骤比较简单，只需要创建一个名为yourname.github.io的repo即可。（yourname为你的github账户名） ","date":"2025-01-12","objectID":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:1:0","tags":["tools"],"title":"使用Hugo构建个人博客","uri":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"2 安装和使用Hugo 安装Hugo：Installation | Hugo （建议下载extended版本） 将hugo.exe所在目录添加到环境变量 新建Hugo网站 hugo new site my_site 下载主题：（此处以LoveIt为例） git clone https://github.com/dillonzq/LoveIt.git my_site/themes/LoveIt 配置config.toml （可以将my_site/themes/LoveIt/exampleSite/config.toml复制到my_site/目录，并自行修改） 新建文章 cd my_site hugo new posts/first_post.md 在本地启动网站 hugo serve 去查看 http://localhost:1313 ","date":"2025-01-12","objectID":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:0","tags":["tools"],"title":"使用Hugo构建个人博客","uri":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"3 部署到Github Pages 构建网站 hugo 会生成一个 public 目录, 其中包含你网站的所有静态内容和资源。 将public初始化为git库，并提交至github ","date":"2025-01-12","objectID":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:3:0","tags":["tools"],"title":"使用Hugo构建个人博客","uri":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"X 参考 Stilig’s blog Yulin Lewis’ blog ","date":"2025-01-12","objectID":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:4:0","tags":["tools"],"title":"使用Hugo构建个人博客","uri":"/%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"}]